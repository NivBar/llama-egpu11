{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83797ea5-91f0-41ea-9c72-615d4fa76044",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a056e4d3-caf2-4183-927f-e6a1638c905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain import LLMChain, HuggingFacePipeline, PromptTemplate\n",
    "from post_process import *\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c4d790-83d5-46a2-b842-ebd45b136be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18860614-77f0-4c91-8ba3-d23ac0dc82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA Available:\", cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3e544d-c072-4994-970d-f7bd7dd99b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24d1f81d7a240539ac8be27cc4cedea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "token = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, use_auth_token=token,use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_quant_type=\"nf8\",\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True, \n",
    "    use_auth_token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True,\n",
    "    use_flash_attention_2=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb397366-c843-4240-9bde-a0d1fb6cc10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "model = model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c533bf6-1b58-4127-aa63-b7a9f2afe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"bot_followup_files/part_1.csv\"\n",
    "orig = pd.read_csv(file_name)\n",
    "p_df = orig[orig.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2b3bdc-2b33-43b9-9870-8de95b64367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = p_df.prompt.to_list()\n",
    "max_seq_len = max(len(tokenizer.encode(p)) for p in passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f9b644-df7f-4d14-891b-69660d762fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 2\n",
    "prompt_batches = [passages[i:i + max_batch_size] for i in range(0, len(passages), max_batch_size)]\n",
    "index_batches = [p_df.index[i:i + max_batch_size].tolist() for i in range(0, len(passages), max_batch_size)]\n",
    "assert len(prompt_batches[0]) == max_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8ce224-5c7d-44e9-a4fd-8a1c51707fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[108, 109]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0833c77b-d2a1-433a-8aa4-ae66bda096cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    # max_length=max_seq_len + 350,\n",
    "    max_new_tokens = 300,\n",
    "    # min_length=max_seq_len + 250, \n",
    "    min_new_tokens = 200,\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.9, \n",
    "    temperature=0.1,  \n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "# template = \"\"\"{text}\"\"\"\n",
    "# prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ad9c2a-f1c2-4965-84b2-74b1340ca112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline model device:\", pipeline.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a437c9-01fe-4dc9-8883-8dacf9cdc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_(responses):\n",
    "    res = []\n",
    "    for i in range(len(responses)):\n",
    "        new_text = \" \".join(responses[i][0][\"generated_text\"].split(\"[/INST]\")[-1].split('\\n')[1:]).strip()\n",
    "        new_text = count_words_complete_sentences(filter_utf8(new_text)).strip()\n",
    "        new_text = re.sub(r'^[^a-zA-Z0-9]+', '', new_text)\n",
    "        res.append(new_text)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ea3b6-4e66-417c-b37f-b9ba0a52ee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1434 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    for batch, idx_batch in tqdm(zip(prompt_batches,index_batches), total=len(prompt_batches)):\n",
    "        responses = pipeline(batch)\n",
    "        texts = post_process_(responses)\n",
    "        for i in range(len(idx_batch)):\n",
    "            orig.at[idx_batch[i], \"text\"] = texts[i]\n",
    "        orig.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a1369-201d-4f4d-9431-7843c480c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# k = 8\n",
    "# # Load the CSV file\n",
    "# csv_file_path = '/lv_local/home/niv.b/llama/bot_followup_files/bot_followup_llama1.csv'  # Replace with your CSV file path\n",
    "# output_folder =  '/lv_local/home/niv.b/llama/bot_followup_files'  # Replace with your desired output folder\n",
    "\n",
    "# # Read the CSV file and split it into k parts\n",
    "# # df = pd.read_csv(csv_file_path)\n",
    "# for i,chunk in enumerate(pd.read_csv(csv_file_path, chunksize=df.shape[0]//k)):\n",
    "#     chunk.to_csv(f'{output_folder}/part_{i+1}.csv'.format(i), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e5ee4-65eb-4e7a-95d4-895eabd1a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n",
    "# output_folder = '/lv_local/home/niv.b/llama/bot_followup_files'  # Replace with the folder containing the split files\n",
    "# concatenated_file_path = '/lv_local/home/niv.b/llama/bot_followup_files/complete_followp.csv'  # Replace with the desired path for the concatenated file\n",
    "\n",
    "# # Concatenate all parts into one DataFrame and save to CSV\n",
    "# pd.concat([pd.read_csv(os.path.join(output_folder, f'part_{i+1}.csv')) for i in range(k)]).to_csv(concatenated_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3c7d0-7d9a-4118-9bab-cdefd363ff81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
